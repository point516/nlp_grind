{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL69zGpmx01k"
      },
      "source": [
        "# Contact\n",
        "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "- Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "- Eleonora Mancini -> e.mancini@unibo.it\n",
        "\n",
        "Professor:\n",
        "- Paolo Torroni -> p.torroni@unibo.it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55jnW-xKxi-2"
      },
      "source": [
        "# Introduction\n",
        "You are asked to address the [EXIST 2023 Task 1](https://clef2023.clef-initiative.eu/index.php?page=Pages/labs.html#EXIST) on sexism detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HWp5bGwySsb"
      },
      "source": [
        "## Problem Definition\n",
        "The systems have to decide whether or not a given tweet contains or describes sexist expressions or behaviors (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behavior).\n",
        "\n",
        "### Examples:\n",
        "\n",
        "**Text**: *Can’t go a day without women womening*\n",
        "\n",
        "**Label**: Sexist\n",
        "\n",
        "**Text**: *''Society's set norms! Happy men's day though!#weareequal''*\n",
        "\n",
        "**Label**: Not sexist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iu1X4I98M8B"
      },
      "source": [
        "#[Task 1 - 1.0 points] Corpus\n",
        "\n",
        "We have preparared a small version of EXIST dataset in our dedicated [Github repository](https://github.com/lt-nlp-lab-unibo/nlp-course-material/tree/main/2024-2025/Assignment%201/data).\n",
        "\n",
        "Check the `A1/data` folder. It contains 3 `.json` files representing `training`, `validation` and `test` sets.\n",
        "\n",
        "The three sets are slightly unbalanced, with a bias toward the `Non-sexist` class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AASoMV9XN5l6"
      },
      "source": [
        "### Dataset Description\n",
        "- The dataset contains tweets in both English and Spanish.\n",
        "- There are labels for multiple tasks, but we are focusing on **Task 1**.\n",
        "- For Task 1, soft labels are assigned by six annotators.\n",
        "- The labels for Task 1 represent whether the tweet is sexist (\"YES\") or not (\"NO\").\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFjwB_lCOQKj"
      },
      "source": [
        "\n",
        "### Example\n",
        "\n",
        "\n",
        "    \"203260\": {\n",
        "        \"id_EXIST\": \"203260\",\n",
        "        \"lang\": \"en\",\n",
        "        \"tweet\": \"ik when mandy says “you look like a whore” i look cute as FUCK\",\n",
        "        \"number_annotators\": 6,\n",
        "        \"annotators\": [\"Annotator_473\", \"Annotator_474\", \"Annotator_475\", \"Annotator_476\", \"Annotator_477\", \"Annotator_27\"],\n",
        "        \"gender_annotators\": [\"F\", \"F\", \"M\", \"M\", \"M\", \"F\"],\n",
        "        \"age_annotators\": [\"18-22\", \"23-45\", \"18-22\", \"23-45\", \"46+\", \"46+\"],\n",
        "        \"labels_task1\": [\"YES\", \"YES\", \"YES\", \"NO\", \"YES\", \"YES\"],\n",
        "        \"labels_task2\": [\"DIRECT\", \"DIRECT\", \"REPORTED\", \"-\", \"JUDGEMENTAL\", \"REPORTED\"],\n",
        "        \"labels_task3\": [\n",
        "          [\"STEREOTYPING-DOMINANCE\"],\n",
        "          [\"OBJECTIFICATION\"],\n",
        "          [\"SEXUAL-VIOLENCE\"],\n",
        "          [\"-\"],\n",
        "          [\"STEREOTYPING-DOMINANCE\", \"OBJECTIFICATION\"],\n",
        "          [\"OBJECTIFICATION\"]\n",
        "        ],\n",
        "        \"split\": \"TRAIN_EN\"\n",
        "      }\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ45bvuOOJ7I"
      },
      "source": [
        "### Instructions\n",
        "1. **Download** the `A1/data` folder.\n",
        "2. **Load** the three JSON files and encode them as pandas dataframes.\n",
        "3. **Generate hard labels** for Task 1 using majority voting and store them in a new dataframe column called `hard_label_task1`. Items without a clear majority will be removed from the dataset.\n",
        "4. **Filter the DataFrame** to keep only rows where the `lang` column is `'en'`.\n",
        "5. **Remove unwanted columns**: Keep only `id_EXIST`, `lang`, `tweet`, and `hard_label_task1`.\n",
        "6. **Encode the `hard_label_task1` column**: Use 1 to represent \"YES\" and 0 to represent \"NO\"."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4j9sbRYE9gl",
        "outputId": "9223cc14-7051-4852-c599-37ae56571ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "BNhCjpRiE_fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading JSON data from a file\n",
        "def encode_json(file_path):\n",
        "  with open(file_path, 'r', encoding='utf-8') as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "  df = pd.DataFrame.from_dict(data, orient='index')\n",
        "  return df\n",
        "\n",
        "# Convert dictionary to pandas DataFrame\n",
        "training = encode_json('/content/drive/MyDrive/assignment-2425/data/training.json')\n",
        "validation = encode_json('/content/drive/MyDrive/assignment-2425/data/validation.json')\n",
        "test = encode_json('/content/drive/MyDrive/assignment-2425/data/test.json')"
      ],
      "metadata": {
        "id": "6gXM9Kp6OB0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels(soft_labels):\n",
        "  labels = []\n",
        "  for row in soft_labels['labels_task1']:\n",
        "    if row.count('YES') > row.count('NO'):\n",
        "      labels.append('YES')\n",
        "    elif row.count('NO') > row.count('YES'):\n",
        "      labels.append('NO')\n",
        "    else:\n",
        "      labels.append(None)\n",
        "\n",
        "  return labels\n",
        "\n",
        "training['hard_label_task1'] = get_labels(training)\n",
        "validation['hard_label_task1'] = get_labels(validation)\n",
        "test['hard_label_task1'] = get_labels(test)\n",
        "\n",
        "training.dropna(inplace=True)\n",
        "validation.dropna(inplace=True)\n",
        "test.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "6gvYlPJjGSK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training = training[training['lang'] == 'en']\n",
        "validation = validation[validation['lang'] == 'en']\n",
        "test = test[test['lang'] == 'en']"
      ],
      "metadata": {
        "id": "E5qm1B46QNI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training = training.loc[:,['id_EXIST', 'lang', 'tweet', 'hard_label_task1']]\n",
        "validation = validation.loc[:,['id_EXIST', 'lang', 'tweet', 'hard_label_task1']]\n",
        "test = test.loc[:,['id_EXIST', 'lang', 'tweet', 'hard_label_task1']]"
      ],
      "metadata": {
        "id": "-sF5ija_QmMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training.loc[training['hard_label_task1'] == 'YES','hard_label_task1'] = 1\n",
        "training.loc[training['hard_label_task1'] == 'NO','hard_label_task1'] = 0\n",
        "\n",
        "validation.loc[validation['hard_label_task1'] == 'YES','hard_label_task1'] = 1\n",
        "validation.loc[validation['hard_label_task1'] == 'NO','hard_label_task1'] = 0\n",
        "\n",
        "test.loc[test['hard_label_task1'] == 'YES','hard_label_task1'] = 1\n",
        "test.loc[test['hard_label_task1'] == 'NO','hard_label_task1'] = 0"
      ],
      "metadata": {
        "id": "oSBD9RYVT2MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "KOy0zxFg_HhI"
      },
      "source": [
        "# [Task2 - 0.5 points] Data Cleaning\n",
        "In the context of tweets, we have noisy and informal data that often includes unnecessary elements like emojis, hashtags, mentions, and URLs. These elements may interfere with the text analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "i0mLIhZf_HhJ"
      },
      "source": [
        "\n",
        "### Instructions\n",
        "- **Remove emojis** from the tweets.\n",
        "- **Remove hashtags** (e.g., `#example`).\n",
        "- **Remove mentions** such as `@user`.\n",
        "- **Remove URLs** from the tweets.\n",
        "- **Remove special characters and symbols**.\n",
        "- **Remove specific quote characters** (e.g., curly quotes).\n",
        "- **Perform lemmatization** to reduce words to their base form."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_tweet(tweet):\n",
        "    # Remove emojis\n",
        "    tweet = re.sub(r'[^\\x00-\\x7F]+', '', tweet)\n",
        "\n",
        "    # Remove hashtags\n",
        "    tweet = re.sub(r'#\\w+', '', tweet)\n",
        "\n",
        "    # Remove mentions\n",
        "    tweet = re.sub(r'@\\w+', '', tweet)\n",
        "\n",
        "    # Remove URLs\n",
        "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove special characters and symbols\n",
        "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
        "\n",
        "    # Remove specific quote characters\n",
        "    tweet = re.sub(r'[“”‘’\"\\']', '', tweet)\n",
        "\n",
        "    # Remove excessive whitespaces\n",
        "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
        "\n",
        "    return tweet.strip().lower()\n",
        "\n",
        "training['tweet'] = training['tweet'].apply(clean_tweet)\n",
        "validation['tweet'] = validation['tweet'].apply(clean_tweet)\n",
        "test['tweet'] = test['tweet'].apply(clean_tweet)\n",
        "\n",
        "train_texts = training['tweet'].values\n",
        "val_texts = validation['tweet'].values\n",
        "test_texts = test['tweet'].values\n",
        "\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "tokenizer = WhitespaceTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_key(pos_tag):\n",
        "    if pos_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif pos_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif pos_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif pos_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return 'n'\n",
        "\n",
        "def lem_text(text: str):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    words = [lemmatizer.lemmatize(word, get_wordnet_key(tag)) for word, tag in tagged]\n",
        "    return \" \".join(words)\n",
        "\n",
        "lem_train_texts = [lem_text(text) for text in tqdm(train_texts, leave=True, position=0)]\n",
        "lem_val_texts = [lem_text(text) for text in tqdm(val_texts, leave=True, position=0)]\n",
        "lem_test_texts = [lem_text(text) for text in tqdm(test_texts, leave=True, position=0)]\n",
        "\n",
        "training['lemmatized'] = lem_train_texts\n",
        "validation['lemmatized'] = lem_val_texts\n",
        "test['lemmatized'] = lem_test_texts"
      ],
      "metadata": {
        "id": "xs1fQP77LqOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "534c5ae8-b252-444b-dda7-076f93549910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "100%|██████████| 2870/2870 [00:07<00:00, 364.02it/s]\n",
            "100%|██████████| 158/158 [00:00<00:00, 935.87it/s]\n",
            "100%|██████████| 286/286 [00:00<00:00, 886.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3KylLHNl0bE"
      },
      "source": [
        "# [Task 3 - 0.5 points] Text Encoding\n",
        "To train a neural sexism classifier, you first need to encode text into numerical format.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr1lTHUVOXff"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Embed words using **GloVe embeddings**.\n",
        "* You are **free** to pick any embedding dimension.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6NNMEjWOZQr"
      },
      "source": [
        "### Note : What about OOV tokens?\n",
        "   * All the tokens in the **training** set that are not in GloVe **must** be added to the vocabulary.\n",
        "   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **special token** (e.g., [UNK]) and a **static** embedding.\n",
        "   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, etc...)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90UztlGUObXk"
      },
      "source": [
        "### More about OOV\n",
        "\n",
        "For a given token:\n",
        "\n",
        "* **If in train set**: add to vocabulary and assign an embedding (use GloVe if token in GloVe, custom embedding otherwise).\n",
        "* **If in val/test set**: assign special token if not in vocabulary and assign custom embedding.\n",
        "\n",
        "Your vocabulary **should**:\n",
        "\n",
        "* Contain all tokens in train set; or\n",
        "* Union of tokens in train set and in GloVe $\\rightarrow$ we make use of existing knowledge!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "\n",
        "embedding_dimension = 100\n",
        "glove = gloader.load(\"glove-wiki-gigaword-{}\".format(embedding_dimension))"
      ],
      "metadata": {
        "id": "WvYY2FuvjYzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ceb0708-4242-4c61-b6fe-328dbef76259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary(df):\n",
        "    idx_to_word = OrderedDict()\n",
        "    word_to_idx = OrderedDict()\n",
        "\n",
        "    # start from 2 because index 0 will be for padding, 1 for <OOV>\n",
        "    curr_idx = 2\n",
        "    for sentence in df:\n",
        "        tokens = sentence.split()\n",
        "        for token in tokens:\n",
        "            if token not in word_to_idx:\n",
        "                word_to_idx[token] = curr_idx\n",
        "                idx_to_word[curr_idx] = token\n",
        "                curr_idx += 1\n",
        "\n",
        "    word_listing = list(idx_to_word.values())\n",
        "    return idx_to_word, word_to_idx, word_listing\n",
        "\n",
        "idx_to_word, word_to_idx, word_listing = build_vocabulary(lem_train_texts)\n",
        "#add 'UNK' token for the OOV tokens in val and test sets\n",
        "idx_to_word[1] = 'UNK'\n",
        "word_to_idx['UNK'] = 1\n",
        "word_listing.append('UNK')\n",
        "print(f\"\\n{len(word_listing)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mhy91mJK2K-V",
        "outputId": "918a2bfe-b84b-440f-8531-ff99475154cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "9841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizing the tweets\n",
        "def tokenize_and_map(text, word_to_index):\n",
        "\n",
        "    tokens = text.split()\n",
        "\n",
        "    # 4. Map tokens to indices, use 'UNK' index if not found\n",
        "    indices = [word_to_index.get(token, word_to_index['UNK']) for token in tokens]\n",
        "\n",
        "    return indices\n",
        "\n",
        "training['tokenized'] = training['lemmatized'].apply(lambda x: tokenize_and_map(x, word_to_idx))\n",
        "validation['tokenized'] = validation['lemmatized'].apply(lambda x: tokenize_and_map(x, word_to_idx))\n",
        "test['tokenized'] = test['lemmatized'].apply(lambda x: tokenize_and_map(x, word_to_idx))"
      ],
      "metadata": {
        "id": "kKWccpFdTze8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_OOV_terms(embedding_model,\n",
        "                    word_listing):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    embedding_vocabulary = set(embedding_model.key_to_index.keys())\n",
        "    oov = set(word_listing).difference(embedding_vocabulary)\n",
        "    return list(oov)\n",
        "\n",
        "oov_terms = check_OOV_terms(glove, word_listing)\n",
        "oov_percentage = float(len(oov_terms)) * 100 / len(word_listing)\n",
        "print(f\"Total OOV terms: {len(oov_terms)} ({oov_percentage:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or48sukUnj0W",
        "outputId": "879c8fe6-92c8-43de-d6b4-cb83fa817f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total OOV terms: 1865 (18.95%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "random embeddings"
      ],
      "metadata": {
        "id": "2DYRIcnHFmjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_matrix(embedding_model,\n",
        "                           embedding_dimension,\n",
        "                           word_to_idx,\n",
        "                           vocab_size,\n",
        "                           oov_terms):\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[word]\n",
        "        except (KeyError, TypeError):\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "MpyWAwIdCRSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = build_embedding_matrix(glove, embedding_dimension, word_to_idx, len(word_to_idx)+1, oov_terms)\n",
        "np.save('/content/drive/MyDrive/assignment-2425/embed.npy', embedding_matrix)\n",
        "print(f\"\\nEmbedding matrix shape: {embedding_matrix.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kETkcEOgERV2",
        "outputId": "cc01200f-4727-47c0-bf7b-6e784d04e1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9841/9841 [00:00<00:00, 292055.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Embedding matrix shape: (9842, 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JLnuLGHGAUT"
      },
      "source": [
        "# [Task 4 - 1.0 points] Model definition\n",
        "\n",
        "You are now tasked to define your sexism classifier.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQFI9J-JOfXD"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
        "* You are **free** to experiment with hyper-parameters to define the baseline model.\n",
        "\n",
        "* **Model 1**: add an additional LSTM layer to the Baseline model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jALc_qYGS2E"
      },
      "source": [
        "### Token to embedding mapping\n",
        "\n",
        "You can follow two approaches for encoding tokens in your classifier.\n",
        "\n",
        "### Work directly with embeddings\n",
        "\n",
        "- Compute the embedding of each input token\n",
        "- Feed the mini-batches of shape (batch_size, # tokens, embedding_dim) to your model\n",
        "\n",
        "### Work with Embedding layer\n",
        "\n",
        "- Encode input tokens to token ids\n",
        "- Define a Embedding layer as the first layer of your model\n",
        "- Compute the embedding matrix of all known tokens (i.e., tokens in your vocabulary)\n",
        "- Initialize the Embedding layer with the computed embedding matrix\n",
        "- You are **free** to set the Embedding layer trainable or not"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Need to pad tokenized sequences to max_length, 60 is the max_length in training\n",
        "max_len = 60\n",
        "training['padded'] = pad_sequences(training['tokenized'], maxlen=max_len, padding='post').tolist()\n",
        "validation['padded'] = pad_sequences(validation['tokenized'], maxlen=max_len, padding='post').tolist()\n",
        "test['padded'] = pad_sequences(test['tokenized'], maxlen=max_len, padding='post').tolist()"
      ],
      "metadata": {
        "id": "lNqZmuybU6rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.load('/content/drive/MyDrive/assignment-2425/embed.npy')\n",
        "vocab_size = embedding_matrix.shape[0]\n",
        "\n",
        "baseline = Sequential([\n",
        "    Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dimension,\n",
        "        weights=[embedding_matrix],  # Load the pre-trained embeddings\n",
        "        mask_zero=True,\n",
        "        trainable=False               # Set to False to keep embeddings fixed\n",
        "    ),\n",
        "    Bidirectional(LSTM(64, return_sequences=False)),\n",
        "    Dense(1, activation='sigmoid')  # For binary classification\n",
        "])\n",
        "\n",
        "model_1 = Sequential([\n",
        "    Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dimension,\n",
        "        weights=[embedding_matrix],  # Load the pre-trained embeddings\n",
        "        mask_zero=True,\n",
        "        trainable=False               # Set to False to keep embeddings fixed\n",
        "    ),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Bidirectional(LSTM(64)), # add another LSTM layer\n",
        "    Dense(1, activation='sigmoid')  # For binary classification\n",
        "])"
      ],
      "metadata": {
        "id": "0axd1zrQJlio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEQTPu6eGgGv"
      },
      "source": [
        "### Padding\n",
        "\n",
        "Pay attention to padding tokens!\n",
        "\n",
        "Your model **should not** be penalized on those tokens.\n",
        "\n",
        "#### How to?\n",
        "\n",
        "There are two main ways.\n",
        "\n",
        "However, their implementation depends on the neural library you are using.\n",
        "\n",
        "- Embedding layer\n",
        "- Custom loss to compute average cross-entropy on non-padding tokens only\n",
        "\n",
        "**Note**: This is a **recommendation**, but we **do not penalize** for missing workarounds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFjBgdiRG3wD"
      },
      "source": [
        "# [Task 5 - 1.0 points] Training and Evaluation\n",
        "\n",
        "You are now tasked to train and evaluate the Baseline and Model 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWPK4umGOjtT"
      },
      "source": [
        "\n",
        "### Instructions\n",
        "\n",
        "* Train **all** models on the train set.\n",
        "* Evaluate **all** models on the validation set.\n",
        "* Compute metrics on the validation set.\n",
        "* Pick **at least** three seeds for robust estimation.\n",
        "* Pick the **best** performing model according to the observed validation set performance.\n",
        "* Evaluate your models using macro F1-score."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(training['padded'].to_list()).astype('float32')\n",
        "y_train = training['hard_label_task1'].values.astype('float32')\n",
        "\n",
        "x_val = np.array(validation['padded'].to_list()).astype('float32')\n",
        "y_val = validation['hard_label_task1'].values.astype('float32')"
      ],
      "metadata": {
        "id": "q-wOJRlkzjud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.expand_dims(y_val,1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlOahTjXmm-z",
        "outputId": "26c532b0-bf2b-4494-a221-a7a9b0f93942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(158, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_values = [1, 42 , 7]\n",
        "\n",
        "for seed in seed_values:\n",
        "  print(f\"model_{seed}\")\n",
        "  tf.random.set_seed(seed)\n",
        "\n",
        "  optim = tf.keras.optimizers.AdamW(learning_rate=0.001)\n",
        "  reduce_LR = tf.keras.callbacks.ReduceLROnPlateau(patience=4)\n",
        "  checkpoint_baseline = tf.keras.callbacks.ModelCheckpoint(f'/content/drive/MyDrive/assignment-2425/baseline_trainable_{seed}.weights.h5', save_weights_only=True, save_best_only=True)\n",
        "\n",
        "  baseline = Sequential([\n",
        "    Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dimension,\n",
        "        weights=[embedding_matrix],  # Load the pre-trained embeddings\n",
        "        mask_zero=True,\n",
        "        trainable=True               # Set to False to keep embeddings fixed\n",
        "    ),\n",
        "    Bidirectional(LSTM(64, return_sequences=False)),\n",
        "    Dense(1, activation='sigmoid')  # For binary classification\n",
        "])\n",
        "\n",
        "  baseline.compile(\n",
        "      loss='binary_crossentropy',\n",
        "      optimizer=optim,\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  history_baseline = baseline.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[reduce_LR, checkpoint_baseline]\n",
        "  )\n",
        "\n",
        "  baseline.load_weights(f'/content/drive/MyDrive/assignment-2425/baseline_trainable_{seed}.weights.h5')\n",
        "  y_pred = baseline.predict(x_val)\n",
        "  f1 = tf.keras.metrics.F1Score(threshold=0.5, average='macro')\n",
        "  f1.update_state(np.expand_dims(y_val,1).astype('int32'), y_pred)\n",
        "  print(f1.result())"
      ],
      "metadata": {
        "id": "yyRDspfAos_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b669d725-eafe-452e-cde5-b4903f57cd4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1\n",
            "Epoch 1/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6160 - loss: 0.6470 - val_accuracy: 0.6456 - val_loss: 0.7049 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7367 - loss: 0.5331 - val_accuracy: 0.7342 - val_loss: 0.7174 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8140 - loss: 0.4255 - val_accuracy: 0.7658 - val_loss: 0.5174 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8698 - loss: 0.3092 - val_accuracy: 0.7722 - val_loss: 0.5040 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9160 - loss: 0.2141 - val_accuracy: 0.8038 - val_loss: 0.5691 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9557 - loss: 0.1209 - val_accuracy: 0.7722 - val_loss: 0.5630 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9763 - loss: 0.0690 - val_accuracy: 0.7975 - val_loss: 0.6289 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9841 - loss: 0.0488 - val_accuracy: 0.7975 - val_loss: 0.9629 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9911 - loss: 0.0346 - val_accuracy: 0.8228 - val_loss: 0.9437 - learning_rate: 1.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9937 - loss: 0.0234 - val_accuracy: 0.8228 - val_loss: 0.9649 - learning_rate: 1.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9953 - loss: 0.0198 - val_accuracy: 0.8165 - val_loss: 0.9807 - learning_rate: 1.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9993 - loss: 0.0171 - val_accuracy: 0.8101 - val_loss: 0.9983 - learning_rate: 1.0000e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9995 - loss: 0.0153 - val_accuracy: 0.8101 - val_loss: 0.9911 - learning_rate: 1.0000e-05\n",
            "Epoch 14/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9995 - loss: 0.0148 - val_accuracy: 0.8165 - val_loss: 0.9871 - learning_rate: 1.0000e-05\n",
            "Epoch 15/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9995 - loss: 0.0144 - val_accuracy: 0.8165 - val_loss: 0.9857 - learning_rate: 1.0000e-05\n",
            "Epoch 16/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0142 - val_accuracy: 0.8165 - val_loss: 0.9858 - learning_rate: 1.0000e-05\n",
            "Epoch 17/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 0.0139 - val_accuracy: 0.8165 - val_loss: 0.9859 - learning_rate: 1.0000e-06\n",
            "Epoch 18/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9998 - loss: 0.0139 - val_accuracy: 0.8165 - val_loss: 0.9859 - learning_rate: 1.0000e-06\n",
            "Epoch 19/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9998 - loss: 0.0139 - val_accuracy: 0.8165 - val_loss: 0.9859 - learning_rate: 1.0000e-06\n",
            "Epoch 20/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 0.0139 - val_accuracy: 0.8165 - val_loss: 0.9860 - learning_rate: 1.0000e-06\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "tf.Tensor(0.7428571, shape=(), dtype=float32)\n",
            "model_42\n",
            "Epoch 1/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6275 - loss: 0.6469 - val_accuracy: 0.7025 - val_loss: 0.6311 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7652 - loss: 0.4916 - val_accuracy: 0.7468 - val_loss: 0.5871 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8387 - loss: 0.3759 - val_accuracy: 0.7785 - val_loss: 0.5248 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8912 - loss: 0.2656 - val_accuracy: 0.7785 - val_loss: 0.5913 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9421 - loss: 0.1636 - val_accuracy: 0.7722 - val_loss: 0.8334 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9641 - loss: 0.0997 - val_accuracy: 0.7848 - val_loss: 0.5023 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9715 - loss: 0.0827 - val_accuracy: 0.7848 - val_loss: 0.7680 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9917 - loss: 0.0421 - val_accuracy: 0.7911 - val_loss: 0.7643 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9963 - loss: 0.0201 - val_accuracy: 0.8038 - val_loss: 0.7716 - learning_rate: 0.0010\n",
            "Epoch 10/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9977 - loss: 0.0117 - val_accuracy: 0.8101 - val_loss: 0.8938 - learning_rate: 0.0010\n",
            "Epoch 11/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9991 - loss: 0.0070 - val_accuracy: 0.7911 - val_loss: 0.9653 - learning_rate: 1.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9997 - loss: 0.0054 - val_accuracy: 0.7911 - val_loss: 0.9884 - learning_rate: 1.0000e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9997 - loss: 0.0051 - val_accuracy: 0.7975 - val_loss: 1.0035 - learning_rate: 1.0000e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9997 - loss: 0.0048 - val_accuracy: 0.8038 - val_loss: 1.0164 - learning_rate: 1.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9999 - loss: 0.0045 - val_accuracy: 0.8038 - val_loss: 1.0173 - learning_rate: 1.0000e-05\n",
            "Epoch 16/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9999 - loss: 0.0045 - val_accuracy: 0.8038 - val_loss: 1.0185 - learning_rate: 1.0000e-05\n",
            "Epoch 17/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9999 - loss: 0.0044 - val_accuracy: 0.8038 - val_loss: 1.0199 - learning_rate: 1.0000e-05\n",
            "Epoch 18/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9999 - loss: 0.0044 - val_accuracy: 0.8038 - val_loss: 1.0213 - learning_rate: 1.0000e-05\n",
            "Epoch 19/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9999 - loss: 0.0043 - val_accuracy: 0.8038 - val_loss: 1.0215 - learning_rate: 1.0000e-06\n",
            "Epoch 20/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0043 - val_accuracy: 0.8038 - val_loss: 1.0217 - learning_rate: 1.0000e-06\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "tf.Tensor(0.72580636, shape=(), dtype=float32)\n",
            "model_7\n",
            "Epoch 1/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.6159 - loss: 0.6685 - val_accuracy: 0.6709 - val_loss: 0.6018 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7500 - loss: 0.5229 - val_accuracy: 0.7468 - val_loss: 0.4998 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8320 - loss: 0.3875 - val_accuracy: 0.7848 - val_loss: 0.4417 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8717 - loss: 0.3027 - val_accuracy: 0.7785 - val_loss: 0.4924 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9323 - loss: 0.1772 - val_accuracy: 0.8038 - val_loss: 0.5886 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9678 - loss: 0.0957 - val_accuracy: 0.7785 - val_loss: 0.6929 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9823 - loss: 0.0605 - val_accuracy: 0.7785 - val_loss: 0.5813 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9881 - loss: 0.0434 - val_accuracy: 0.7722 - val_loss: 0.6627 - learning_rate: 1.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9883 - loss: 0.0353 - val_accuracy: 0.7722 - val_loss: 0.7169 - learning_rate: 1.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9915 - loss: 0.0303 - val_accuracy: 0.7722 - val_loss: 0.7547 - learning_rate: 1.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9922 - loss: 0.0262 - val_accuracy: 0.7722 - val_loss: 0.7900 - learning_rate: 1.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9948 - loss: 0.0233 - val_accuracy: 0.7848 - val_loss: 0.7711 - learning_rate: 1.0000e-05\n",
            "Epoch 13/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9926 - loss: 0.0225 - val_accuracy: 0.7848 - val_loss: 0.7656 - learning_rate: 1.0000e-05\n",
            "Epoch 14/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9926 - loss: 0.0221 - val_accuracy: 0.7848 - val_loss: 0.7660 - learning_rate: 1.0000e-05\n",
            "Epoch 15/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9926 - loss: 0.0217 - val_accuracy: 0.7848 - val_loss: 0.7689 - learning_rate: 1.0000e-05\n",
            "Epoch 16/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9926 - loss: 0.0214 - val_accuracy: 0.7848 - val_loss: 0.7691 - learning_rate: 1.0000e-06\n",
            "Epoch 17/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9926 - loss: 0.0214 - val_accuracy: 0.7848 - val_loss: 0.7694 - learning_rate: 1.0000e-06\n",
            "Epoch 18/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9926 - loss: 0.0213 - val_accuracy: 0.7848 - val_loss: 0.7697 - learning_rate: 1.0000e-06\n",
            "Epoch 19/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9926 - loss: 0.0213 - val_accuracy: 0.7848 - val_loss: 0.7700 - learning_rate: 1.0000e-06\n",
            "Epoch 20/20\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9926 - loss: 0.0213 - val_accuracy: 0.7848 - val_loss: 0.7700 - learning_rate: 1.0000e-07\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 110ms/step\n",
            "tf.Tensor(0.73015875, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding(training=False) F1-scores = (0.7633587, 0.76422757, 0.779661)\n",
        "#### Embedding(trainable=True) F1-score = (0.7428571, 0.72580636, 0.73015875)"
      ],
      "metadata": {
        "id": "zPjwnDSBp1hH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_values = [1, 42 , 7]\n",
        "\n",
        "for seed in seed_values:\n",
        "  print(f\"model_{seed}\")\n",
        "  tf.random.set_seed(seed)\n",
        "\n",
        "  model_1 = Sequential([\n",
        "    Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dimension,\n",
        "        weights=[embedding_matrix],  # Load the pre-trained embeddings\n",
        "        mask_zero=True,\n",
        "        trainable=True               # Set to False to keep embeddings fixed\n",
        "    ),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Bidirectional(LSTM(64)), # add another LSTM layer\n",
        "    Dense(1, activation='sigmoid')  # For binary classification\n",
        "  ])\n",
        "\n",
        "  optim = tf.keras.optimizers.AdamW(learning_rate=0.001)\n",
        "  reduce_LR = tf.keras.callbacks.ReduceLROnPlateau(patience=4)\n",
        "  checkpoint_model1 = tf.keras.callbacks.ModelCheckpoint(f'/content/drive/MyDrive/assignment-2425/model1_trainable_{seed}.weights.h5', save_weights_only=True, save_best_only=True)\n",
        "\n",
        "  model_1.compile(\n",
        "      loss='binary_crossentropy',\n",
        "      optimizer=optim,\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  history_model1 = model_1.fit(\n",
        "      x_train,\n",
        "      y_train,\n",
        "      epochs=30,\n",
        "      batch_size=32,\n",
        "      validation_data=(x_val, y_val),\n",
        "      callbacks=[reduce_LR, checkpoint_model1]\n",
        "  )\n",
        "\n",
        "  model_1.load_weights(f'/content/drive/MyDrive/assignment-2425/model1_trainable_{seed}.weights.h5')\n",
        "  y_pred = model_1.predict(x_val)\n",
        "  f1 = tf.keras.metrics.F1Score(threshold=0.5, average='macro')\n",
        "  f1.update_state(np.expand_dims(y_val,1).astype('int32'), y_pred)\n",
        "  print(f1.result())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNI_TmD6rPbk",
        "outputId": "7f35b8ec-fb43-4f68-95d0-81378f31d33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_1\n",
            "Epoch 1/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 29ms/step - accuracy: 0.6153 - loss: 0.6391 - val_accuracy: 0.7089 - val_loss: 0.6802 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.7715 - loss: 0.4936 - val_accuracy: 0.7532 - val_loss: 0.6137 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8398 - loss: 0.3706 - val_accuracy: 0.7785 - val_loss: 0.6186 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.8967 - loss: 0.2592 - val_accuracy: 0.8101 - val_loss: 0.4574 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9361 - loss: 0.1666 - val_accuracy: 0.7342 - val_loss: 0.6106 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9554 - loss: 0.1174 - val_accuracy: 0.8038 - val_loss: 0.6364 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9740 - loss: 0.0730 - val_accuracy: 0.8165 - val_loss: 0.7095 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9900 - loss: 0.0277 - val_accuracy: 0.8165 - val_loss: 1.0369 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9948 - loss: 0.0195 - val_accuracy: 0.8165 - val_loss: 0.9797 - learning_rate: 1.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9963 - loss: 0.0141 - val_accuracy: 0.8165 - val_loss: 1.0144 - learning_rate: 1.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9963 - loss: 0.0126 - val_accuracy: 0.8101 - val_loss: 1.0494 - learning_rate: 1.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9963 - loss: 0.0115 - val_accuracy: 0.8101 - val_loss: 1.0830 - learning_rate: 1.0000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9973 - loss: 0.0105 - val_accuracy: 0.8101 - val_loss: 1.0865 - learning_rate: 1.0000e-05\n",
            "Epoch 14/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9973 - loss: 0.0104 - val_accuracy: 0.8101 - val_loss: 1.0902 - learning_rate: 1.0000e-05\n",
            "Epoch 15/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9973 - loss: 0.0103 - val_accuracy: 0.8101 - val_loss: 1.0943 - learning_rate: 1.0000e-05\n",
            "Epoch 16/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9973 - loss: 0.0102 - val_accuracy: 0.8101 - val_loss: 1.0987 - learning_rate: 1.0000e-05\n",
            "Epoch 17/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9973 - loss: 0.0100 - val_accuracy: 0.8101 - val_loss: 1.0992 - learning_rate: 1.0000e-06\n",
            "Epoch 18/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9973 - loss: 0.0100 - val_accuracy: 0.8101 - val_loss: 1.0997 - learning_rate: 1.0000e-06\n",
            "Epoch 19/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9973 - loss: 0.0100 - val_accuracy: 0.8101 - val_loss: 1.1002 - learning_rate: 1.0000e-06\n",
            "Epoch 20/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9973 - loss: 0.0100 - val_accuracy: 0.8101 - val_loss: 1.1007 - learning_rate: 1.0000e-06\n",
            "Epoch 21/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9973 - loss: 0.0100 - val_accuracy: 0.8101 - val_loss: 1.1008 - learning_rate: 1.0000e-07\n",
            "Epoch 22/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9973 - loss: 0.0100 - val_accuracy: 0.8101 - val_loss: 1.1008 - learning_rate: 1.0000e-07\n",
            "Epoch 23/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9973 - loss: 0.0100 - val_accuracy: 0.8101 - val_loss: 1.1009 - learning_rate: 1.0000e-07\n",
            "Epoch 24/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9973 - loss: 0.0100 - val_accuracy: 0.8101 - val_loss: 1.1010 - learning_rate: 1.0000e-07\n",
            "Epoch 25/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9973 - loss: 0.0100 - val_accuracy: 0.8101 - val_loss: 1.1010 - learning_rate: 1.0000e-08\n",
            "Epoch 26/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9973 - loss: 0.0100 - val_accuracy: 0.8101 - val_loss: 1.1010 - learning_rate: 1.0000e-08\n",
            "Epoch 27/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.9973 - loss: 0.0100 - val_accuracy: 0.8101 - val_loss: 1.1010 - learning_rate: 1.0000e-08\n",
            "Epoch 28/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9973 - loss: 0.0100 - val_accuracy: 0.8101 - val_loss: 1.1010 - learning_rate: 1.0000e-08\n",
            "Epoch 29/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9973 - loss: 0.0100 - val_accuracy: 0.8101 - val_loss: 1.1010 - learning_rate: 1.0000e-09\n",
            "Epoch 30/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9973 - loss: 0.0100 - val_accuracy: 0.8101 - val_loss: 1.1010 - learning_rate: 1.0000e-09\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step\n",
            "tf.Tensor(0.76562494, shape=(), dtype=float32)\n",
            "model_42\n",
            "Epoch 1/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - accuracy: 0.6206 - loss: 0.6343 - val_accuracy: 0.7152 - val_loss: 0.6105 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.7689 - loss: 0.4775 - val_accuracy: 0.7785 - val_loss: 0.5467 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8444 - loss: 0.3583 - val_accuracy: 0.7468 - val_loss: 0.8170 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8865 - loss: 0.2686 - val_accuracy: 0.8165 - val_loss: 0.4825 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9396 - loss: 0.1461 - val_accuracy: 0.8165 - val_loss: 0.5236 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9621 - loss: 0.0973 - val_accuracy: 0.7911 - val_loss: 0.7546 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9760 - loss: 0.0617 - val_accuracy: 0.7911 - val_loss: 0.7990 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9779 - loss: 0.0580 - val_accuracy: 0.7722 - val_loss: 0.5592 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9938 - loss: 0.0278 - val_accuracy: 0.8165 - val_loss: 0.6583 - learning_rate: 1.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9962 - loss: 0.0181 - val_accuracy: 0.8101 - val_loss: 0.6989 - learning_rate: 1.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9973 - loss: 0.0157 - val_accuracy: 0.8101 - val_loss: 0.7326 - learning_rate: 1.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9978 - loss: 0.0139 - val_accuracy: 0.8101 - val_loss: 0.7625 - learning_rate: 1.0000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9980 - loss: 0.0126 - val_accuracy: 0.8101 - val_loss: 0.7648 - learning_rate: 1.0000e-05\n",
            "Epoch 14/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9980 - loss: 0.0125 - val_accuracy: 0.8101 - val_loss: 0.7674 - learning_rate: 1.0000e-05\n",
            "Epoch 15/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9980 - loss: 0.0123 - val_accuracy: 0.8101 - val_loss: 0.7704 - learning_rate: 1.0000e-05\n",
            "Epoch 16/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9980 - loss: 0.0122 - val_accuracy: 0.8101 - val_loss: 0.7735 - learning_rate: 1.0000e-05\n",
            "Epoch 17/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9980 - loss: 0.0121 - val_accuracy: 0.8101 - val_loss: 0.7738 - learning_rate: 1.0000e-06\n",
            "Epoch 18/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9980 - loss: 0.0120 - val_accuracy: 0.8101 - val_loss: 0.7742 - learning_rate: 1.0000e-06\n",
            "Epoch 19/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9980 - loss: 0.0120 - val_accuracy: 0.8101 - val_loss: 0.7746 - learning_rate: 1.0000e-06\n",
            "Epoch 20/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9980 - loss: 0.0120 - val_accuracy: 0.8101 - val_loss: 0.7749 - learning_rate: 1.0000e-06\n",
            "Epoch 21/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9980 - loss: 0.0120 - val_accuracy: 0.8101 - val_loss: 0.7750 - learning_rate: 1.0000e-07\n",
            "Epoch 22/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9980 - loss: 0.0120 - val_accuracy: 0.8101 - val_loss: 0.7750 - learning_rate: 1.0000e-07\n",
            "Epoch 23/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9980 - loss: 0.0120 - val_accuracy: 0.8101 - val_loss: 0.7751 - learning_rate: 1.0000e-07\n",
            "Epoch 24/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9980 - loss: 0.0120 - val_accuracy: 0.8101 - val_loss: 0.7751 - learning_rate: 1.0000e-07\n",
            "Epoch 25/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9980 - loss: 0.0120 - val_accuracy: 0.8101 - val_loss: 0.7751 - learning_rate: 1.0000e-08\n",
            "Epoch 26/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9980 - loss: 0.0120 - val_accuracy: 0.8101 - val_loss: 0.7751 - learning_rate: 1.0000e-08\n",
            "Epoch 27/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9980 - loss: 0.0120 - val_accuracy: 0.8101 - val_loss: 0.7751 - learning_rate: 1.0000e-08\n",
            "Epoch 28/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9980 - loss: 0.0120 - val_accuracy: 0.8101 - val_loss: 0.7751 - learning_rate: 1.0000e-08\n",
            "Epoch 29/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9980 - loss: 0.0120 - val_accuracy: 0.8101 - val_loss: 0.7751 - learning_rate: 1.0000e-09\n",
            "Epoch 30/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9980 - loss: 0.0120 - val_accuracy: 0.8101 - val_loss: 0.7751 - learning_rate: 1.0000e-09\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step\n",
            "tf.Tensor(0.76422757, shape=(), dtype=float32)\n",
            "model_7\n",
            "Epoch 1/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.5957 - loss: 0.6682 - val_accuracy: 0.6835 - val_loss: 0.5697 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.7425 - loss: 0.5089 - val_accuracy: 0.7658 - val_loss: 0.4829 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8393 - loss: 0.3635 - val_accuracy: 0.8165 - val_loss: 0.4915 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9009 - loss: 0.2421 - val_accuracy: 0.8101 - val_loss: 0.5332 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9514 - loss: 0.1346 - val_accuracy: 0.8038 - val_loss: 0.5405 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9692 - loss: 0.0865 - val_accuracy: 0.8038 - val_loss: 0.6311 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9868 - loss: 0.0468 - val_accuracy: 0.8038 - val_loss: 0.7306 - learning_rate: 1.0000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9897 - loss: 0.0339 - val_accuracy: 0.8228 - val_loss: 0.7509 - learning_rate: 1.0000e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9922 - loss: 0.0284 - val_accuracy: 0.8165 - val_loss: 0.7765 - learning_rate: 1.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9946 - loss: 0.0240 - val_accuracy: 0.8101 - val_loss: 0.8002 - learning_rate: 1.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9952 - loss: 0.0203 - val_accuracy: 0.8101 - val_loss: 0.7996 - learning_rate: 1.0000e-05\n",
            "Epoch 12/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9954 - loss: 0.0199 - val_accuracy: 0.8101 - val_loss: 0.8002 - learning_rate: 1.0000e-05\n",
            "Epoch 13/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9954 - loss: 0.0195 - val_accuracy: 0.8101 - val_loss: 0.8023 - learning_rate: 1.0000e-05\n",
            "Epoch 14/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9954 - loss: 0.0192 - val_accuracy: 0.8101 - val_loss: 0.8051 - learning_rate: 1.0000e-05\n",
            "Epoch 15/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9954 - loss: 0.0188 - val_accuracy: 0.8101 - val_loss: 0.8055 - learning_rate: 1.0000e-06\n",
            "Epoch 16/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9956 - loss: 0.0188 - val_accuracy: 0.8101 - val_loss: 0.8058 - learning_rate: 1.0000e-06\n",
            "Epoch 17/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.9956 - loss: 0.0187 - val_accuracy: 0.8101 - val_loss: 0.8062 - learning_rate: 1.0000e-06\n",
            "Epoch 18/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9956 - loss: 0.0187 - val_accuracy: 0.8101 - val_loss: 0.8066 - learning_rate: 1.0000e-06\n",
            "Epoch 19/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9956 - loss: 0.0187 - val_accuracy: 0.8101 - val_loss: 0.8066 - learning_rate: 1.0000e-07\n",
            "Epoch 20/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9956 - loss: 0.0187 - val_accuracy: 0.8101 - val_loss: 0.8066 - learning_rate: 1.0000e-07\n",
            "Epoch 21/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9956 - loss: 0.0187 - val_accuracy: 0.8101 - val_loss: 0.8067 - learning_rate: 1.0000e-07\n",
            "Epoch 22/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9956 - loss: 0.0187 - val_accuracy: 0.8101 - val_loss: 0.8067 - learning_rate: 1.0000e-07\n",
            "Epoch 23/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9956 - loss: 0.0186 - val_accuracy: 0.8101 - val_loss: 0.8067 - learning_rate: 1.0000e-08\n",
            "Epoch 24/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9956 - loss: 0.0186 - val_accuracy: 0.8101 - val_loss: 0.8067 - learning_rate: 1.0000e-08\n",
            "Epoch 25/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9956 - loss: 0.0186 - val_accuracy: 0.8101 - val_loss: 0.8067 - learning_rate: 1.0000e-08\n",
            "Epoch 26/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9956 - loss: 0.0186 - val_accuracy: 0.8101 - val_loss: 0.8067 - learning_rate: 1.0000e-08\n",
            "Epoch 27/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.9956 - loss: 0.0186 - val_accuracy: 0.8101 - val_loss: 0.8067 - learning_rate: 1.0000e-09\n",
            "Epoch 28/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9956 - loss: 0.0186 - val_accuracy: 0.8101 - val_loss: 0.8067 - learning_rate: 1.0000e-09\n",
            "Epoch 29/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9956 - loss: 0.0186 - val_accuracy: 0.8101 - val_loss: 0.8067 - learning_rate: 1.0000e-09\n",
            "Epoch 30/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9956 - loss: 0.0186 - val_accuracy: 0.8101 - val_loss: 0.8067 - learning_rate: 1.0000e-09\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step\n",
            "tf.Tensor(0.6942148, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding(training=False) F1-scores = (0.74999994, 0.73684204, 0.76033056)\n",
        "#### Embedding(trainable=True) F1-score = (0.76562494, 0.76422757, 0.6942148)"
      ],
      "metadata": {
        "id": "ki5k5EcksHIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = Sequential([\n",
        "    Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dimension,\n",
        "        weights=[embedding_matrix],  # Load the pre-trained embeddings\n",
        "        mask_zero=True,\n",
        "        trainable=False               # Set to False to keep embeddings fixed\n",
        "    ),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Bidirectional(LSTM(64)), # add another LSTM layer\n",
        "    Dense(1, activation='sigmoid')  # For binary classification\n",
        "])"
      ],
      "metadata": {
        "id": "wgV1-gL_sIQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optim = tf.keras.optimizers.AdamW(learning_rate=0.001)\n",
        "reduce_LR = tf.keras.callbacks.ReduceLROnPlateau(patience=4)\n",
        "checkpoint_model1 = tf.keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/assignment-2425/model1.weights.h5', save_weights_only=True, save_best_only=True)\n",
        "\n",
        "\n",
        "model_1.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=optim,\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "KT-PcbhgoIF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_model1 = model_1.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[reduce_LR, checkpoint_model1]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePRsKk6FmX3E",
        "outputId": "dd37772c-2a30-4f30-983e-4b8ff9742ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5997 - loss: 0.6634 - val_accuracy: 0.7025 - val_loss: 0.5419 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.7207 - loss: 0.5521 - val_accuracy: 0.7468 - val_loss: 0.5222 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7541 - loss: 0.5077 - val_accuracy: 0.7595 - val_loss: 0.4868 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.8000 - loss: 0.4448 - val_accuracy: 0.7658 - val_loss: 0.4822 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8198 - loss: 0.4096 - val_accuracy: 0.7848 - val_loss: 0.4772 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8372 - loss: 0.3820 - val_accuracy: 0.8101 - val_loss: 0.4415 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8563 - loss: 0.3310 - val_accuracy: 0.7975 - val_loss: 0.5320 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8771 - loss: 0.2851 - val_accuracy: 0.7975 - val_loss: 0.5084 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9223 - loss: 0.2093 - val_accuracy: 0.7722 - val_loss: 0.6342 - learning_rate: 0.0010\n",
            "Epoch 10/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9302 - loss: 0.1848 - val_accuracy: 0.7658 - val_loss: 0.7452 - learning_rate: 0.0010\n",
            "Epoch 11/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9495 - loss: 0.1458 - val_accuracy: 0.7975 - val_loss: 0.6506 - learning_rate: 1.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9780 - loss: 0.0827 - val_accuracy: 0.8101 - val_loss: 0.7013 - learning_rate: 1.0000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9737 - loss: 0.0801 - val_accuracy: 0.8038 - val_loss: 0.7498 - learning_rate: 1.0000e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.9860 - loss: 0.0537 - val_accuracy: 0.7975 - val_loss: 0.7605 - learning_rate: 1.0000e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9892 - loss: 0.0515 - val_accuracy: 0.7975 - val_loss: 0.7755 - learning_rate: 1.0000e-05\n",
            "Epoch 16/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9879 - loss: 0.0508 - val_accuracy: 0.7975 - val_loss: 0.7834 - learning_rate: 1.0000e-05\n",
            "Epoch 17/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9881 - loss: 0.0506 - val_accuracy: 0.7975 - val_loss: 0.7919 - learning_rate: 1.0000e-05\n",
            "Epoch 18/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9871 - loss: 0.0500 - val_accuracy: 0.7975 - val_loss: 0.7949 - learning_rate: 1.0000e-05\n",
            "Epoch 19/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9867 - loss: 0.0516 - val_accuracy: 0.7975 - val_loss: 0.7953 - learning_rate: 1.0000e-06\n",
            "Epoch 20/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.9899 - loss: 0.0500 - val_accuracy: 0.7975 - val_loss: 0.7962 - learning_rate: 1.0000e-06\n",
            "Epoch 21/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.9874 - loss: 0.0532 - val_accuracy: 0.7975 - val_loss: 0.7964 - learning_rate: 1.0000e-06\n",
            "Epoch 22/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9865 - loss: 0.0525 - val_accuracy: 0.7975 - val_loss: 0.7970 - learning_rate: 1.0000e-06\n",
            "Epoch 23/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9888 - loss: 0.0493 - val_accuracy: 0.7975 - val_loss: 0.7971 - learning_rate: 1.0000e-07\n",
            "Epoch 24/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.9894 - loss: 0.0485 - val_accuracy: 0.7975 - val_loss: 0.7971 - learning_rate: 1.0000e-07\n",
            "Epoch 25/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9889 - loss: 0.0502 - val_accuracy: 0.7975 - val_loss: 0.7971 - learning_rate: 1.0000e-07\n",
            "Epoch 26/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9877 - loss: 0.0510 - val_accuracy: 0.7975 - val_loss: 0.7972 - learning_rate: 1.0000e-07\n",
            "Epoch 27/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9894 - loss: 0.0492 - val_accuracy: 0.7975 - val_loss: 0.7972 - learning_rate: 1.0000e-08\n",
            "Epoch 28/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.9865 - loss: 0.0498 - val_accuracy: 0.7975 - val_loss: 0.7972 - learning_rate: 1.0000e-08\n",
            "Epoch 29/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.9879 - loss: 0.0520 - val_accuracy: 0.7975 - val_loss: 0.7972 - learning_rate: 1.0000e-08\n",
            "Epoch 30/30\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - accuracy: 0.9903 - loss: 0.0466 - val_accuracy: 0.7975 - val_loss: 0.7972 - learning_rate: 1.0000e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline.load_weights('/content/drive/MyDrive/assignment-2425/baseline.weights.h5')\n",
        "print(baseline.evaluate(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB3JVYqg0KHz",
        "outputId": "9ae071b1-77b0-4474-d96f-8f6b2c30e159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7855 - loss: 0.4468 \n",
            "[0.44200608134269714, 0.7848101258277893]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.load_weights('/content/drive/MyDrive/assignment-2425/model1.weights.h5')\n",
        "print(model_1.evaluate(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HIxtXGJscw6",
        "outputId": "11993069-60b3-4a26-dee9-30975f144305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8026 - loss: 0.4434\n",
            "[0.4414699971675873, 0.8101266026496887]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSy9sPwYHUoD"
      },
      "source": [
        "# [Task 6 - 1.0 points] Transformers\n",
        "\n",
        "In this section, you will use a transformer model specifically trained for hate speech detection, namely [Twitter-roBERTa-base for Hate Speech Detection](https://huggingface.co/cardiffnlp/twitter-roberta-base-hate).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "njQmpZzH_HhP"
      },
      "source": [
        "### Relevant Material\n",
        "- Tutorial 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "uZTi5m0L_HhQ"
      },
      "source": [
        "### Instructions\n",
        "1. **Load the Tokenizer and Model**\n",
        "\n",
        "2. **Preprocess the Dataset**:\n",
        "   You will need to preprocess your dataset to prepare it for input into the model. Tokenize your text data using the appropriate tokenizer and ensure it is formatted correctly.\n",
        "\n",
        "   **Note**: You have to use the plain text of the dataset and not the version that you tokenized before, as you need to tokenize the cleaned text obtained after the initial cleaning process.\n",
        "\n",
        "3. **Train the Model**:\n",
        "   Use the `Trainer` to train the model on your training data.\n",
        "\n",
        "4. **Evaluate the Model on the Test Set** using F1-macro."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorWithPadding\n",
        "import torch\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "MGUJCJunGMnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = training['lemmatized'].tolist()\n",
        "val_texts = validation['lemmatized'].tolist()\n",
        "test_texts = test['lemmatized'].tolist()\n",
        "\n",
        "train_labels = training['hard_label_task1'].tolist()\n",
        "val_labels = validation['hard_label_task1'].tolist()\n",
        "test_labels = test['hard_label_task1'].tolist()"
      ],
      "metadata": {
        "id": "i5OpB4KrG8od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"cardiffnlp/twitter-roberta-base-hate\"  # Replace with the correct model name\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ],
      "metadata": {
        "id": "JyYID4LQHMdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the training and testing data\n",
        "train_encodings = tokenizer(train_texts)\n",
        "val_encodings = tokenizer(val_texts)\n",
        "test_encodings = tokenizer(test_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq0VyhKdHUEH",
        "outputId": "fb082029-3a4c-4e3d-c913-0ba11018c10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg1pebcdLmHw",
        "outputId": "16247aab-f911-45ae-d4fb-4dbf1d61dbdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=49, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SexismDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = SexismDataset(train_encodings, train_labels)\n",
        "val_dataset = SexismDataset(val_encodings, val_labels)\n",
        "test_dataset = SexismDataset(test_encodings, test_labels)"
      ],
      "metadata": {
        "id": "yKDPJuiuSjm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "QWmCQfupQgiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/assignment-2425/transformer/results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir='/content/drive/MyDrive/assignment-2425/transformer/logs',\n",
        "    load_best_model_at_end=True,\n",
        "    seed = 42\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAKEUL_aNY2V",
        "outputId": "1e41c842-94b1-4702-fcc7-717e7ac00f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        'f1': f1_score(labels, predictions, average='macro')\n",
        "    }"
      ],
      "metadata": {
        "id": "0iowvQLQRuUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0zKqFpIRC5W",
        "outputId": "65b13c16-bd28-4ac1-a807-927de5e3c95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-27b6ee46aeb3>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Yk7qUbgNSLGY",
        "outputId": "9548597a-1d6e-4657-918f-4d773ab094b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1077' max='1077' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1077/1077 03:09, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.322713</td>\n",
              "      <td>0.884221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.425000</td>\n",
              "      <td>0.894403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.225000</td>\n",
              "      <td>0.540179</td>\n",
              "      <td>0.887497</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1077, training_loss=0.31003141270374523, metrics={'train_runtime': 189.7013, 'train_samples_per_second': 45.387, 'train_steps_per_second': 5.677, 'total_flos': 239207004869520.0, 'train_loss': 0.31003141270374523, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = trainer.evaluate()\n",
        "print(f\"F1-Macro Score: {eval_result['eval_f1']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "omWQfi3hT_Y9",
        "outputId": "7410b418-bd58-4b6b-8aa5-58a66cbc5c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Macro Score: 0.8842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gtiG2mAL3HM"
      },
      "source": [
        "# [Task 7 - 0.5 points] Error Analysis\n",
        "\n",
        "### Instructions\n",
        "\n",
        "After evaluating the model, perform a brief error analysis:\n",
        "\n",
        " - Review the results and identify common errors.\n",
        "\n",
        " - Summarize your findings regarding the errors and their impact on performance (e.g. but not limited to Out-of-Vocabulary (OOV) words, data imbalance, and performance differences between the custom model and the transformer...)\n",
        " - Suggest possible solutions to address the identified errors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P42XYjb6K3k5"
      },
      "source": [
        "# [Task 8 - 0.5 points] Report\n",
        "\n",
        "Wrap up your experiment in a short report (up to 2 pages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9oXSaW1K5S7"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "* Use the NLP course report template.\n",
        "* Summarize each task in the report following the provided template."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHw2L6PlLDyE"
      },
      "source": [
        "### Recommendations\n",
        "\n",
        "The report is not a copy-paste of graphs, tables, and command outputs.\n",
        "\n",
        "* Summarize classification performance in Table format.\n",
        "* **Do not** report command outputs or screenshots.\n",
        "* Report learning curves in Figure format.\n",
        "* The error analysis section should summarize your findings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMUqh1utLflM"
      },
      "source": [
        "# Submission\n",
        "\n",
        "* **Submit** your report in PDF format.\n",
        "* **Submit** your python notebook.\n",
        "* Make sure your notebook is **well organized**, with no temporary code, commented sections, tests, etc...\n",
        "* You can upload **model weights** in a cloud repository and report the link in the report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypagJed7LheY"
      },
      "source": [
        "# FAQ\n",
        "\n",
        "Please check this frequently asked questions before contacting us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgtFwKXMLjww"
      },
      "source": [
        "### Execution Order\n",
        "\n",
        "You are **free** to address tasks in any order (if multiple orderings are available)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BjMk5e_M4n7"
      },
      "source": [
        "### Trainable Embeddings\n",
        "\n",
        "You are **free** to define a trainable or non-trainable Embedding layer to load the GloVe embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8TVgpYlM6s5"
      },
      "source": [
        "### Model architecture\n",
        "\n",
        "You **should not** change the architecture of a model (i.e., its layers).\n",
        "However, you are **free** to play with their hyper-parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia6IapI1M_A7"
      },
      "source": [
        "### Neural Libraries\n",
        "\n",
        "You are **free** to use any library of your choice to implement the networks (e.g., Keras, Tensorflow, PyTorch, JAX, etc...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWDaW8DyNBu5"
      },
      "source": [
        "### Keras TimeDistributed Dense layer\n",
        "\n",
        "If you are using Keras, we recommend wrapping the final Dense layer with `TimeDistributed`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1WcrpemNEQm"
      },
      "source": [
        "### Robust Evaluation\n",
        "\n",
        "Each model is trained with at least 3 random seeds.\n",
        "\n",
        "Task 4 requires you to compute the average performance over the 3 seeds and its corresponding standard deviation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mVe5dqzNI_u"
      },
      "source": [
        "### Model Selection for Analysis\n",
        "\n",
        "To carry out the error analysis you are **free** to either\n",
        "\n",
        "* Pick examples or perform comparisons with an individual seed run model (e.g., Baseline seed 1337)\n",
        "* Perform ensembling via, for instance, majority voting to obtain a single model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8a4pDKSNKzI"
      },
      "source": [
        "### Error Analysis\n",
        "\n",
        "Some topics for discussion include:\n",
        "   * Precision/Recall curves.\n",
        "   * Confusion matrices.\n",
        "   * Specific misclassified samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ThjNAPkj_HhS"
      },
      "source": [
        "### Bonus Points\n",
        "Bonus points are arbitrarily assigned based on significant contributions such as:\n",
        "- Outstanding error analysis\n",
        "- Masterclass code organization\n",
        "- Suitable extensions\n",
        "Note that bonus points are only assigned if all task points are attributed (i.e., 6/6).\n",
        "\n",
        "**Possible Extensions/Explorations for Bonus Points:**\n",
        "- **Try other preprocessing strategies**: e.g., but not limited to, explore techniques tailored specifically for tweets or  methods that are common in social media text.\n",
        "- **Experiment with other custom architectures or models from HuggingFace**\n",
        "- **Explore Spanish tweets**: e.g., but not limited to, leverage multilingual models to process Spanish tweets and assess their performance compared to monolingual models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xmMKE7vLu-y"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# The End"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}